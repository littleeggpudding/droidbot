import sys
import argparse
import os
import glob
import csv
from multiprocessing import Pool, cpu_count
from droidbot.utils import generate_html_report


def find_replay_folders(parent_dir, base_app_filter=None):
    """
    åœ¨ parent_dir ä¸‹æŸ¥æ‰¾æ‰€æœ‰ replay_output_*_for_* æ–‡ä»¶å¤¹ã€‚
    å¦‚æœæä¾› base_app_filterï¼ˆä¾‹å¦‚ "3_8_0"ï¼‰ï¼Œåˆ™ä»…è¿”å› *_for_{base_app_filter} çš„ç›®å½•ã€‚
    """
    pattern = os.path.join(parent_dir, "replay_output_*_for_*")
    replay_folders = glob.glob(pattern)

    if base_app_filter:
        suffix = f"_for_{base_app_filter}"
        replay_folders = [p for p in replay_folders if os.path.basename(p).endswith(suffix)]

    return replay_folders


def derive_record_folder(replay_folder_name, parent_dir):
    """
    æ ¹æ® replay æ–‡ä»¶å¤¹åæ¨å¯¼å¯¹åº”çš„ record æ–‡ä»¶å¤¹åï¼Œå¹¶åœ¨ parent_dir ä¸‹æŸ¥æ‰¾ã€‚
    ä¾‹ï¼šreplay_output_6_3_0_run3_for_4_7_2 -> record_output_4_7_2_run3
    è¿”å›ï¼šbasenameï¼ˆä¾‹å¦‚ record_output_4_7_2_run3ï¼‰æˆ– None
    """
    if "_for_" not in replay_folder_name:
        return None

    parts = replay_folder_name.split("_for_")
    if len(parts) != 2:
        return None

    target_version = parts[1]
    base_part = parts[0]
    
    # æå–run_count
    run_count = None
    if "_run" in base_part:
        run_parts = base_part.split("_run")
        if len(run_parts) == 2:
            run_count = run_parts[1]
    
    # å¦‚æœæ‰¾åˆ°äº†run_countï¼Œå°è¯•ç²¾ç¡®åŒ¹é…
    if run_count:
        record_pattern = os.path.join(parent_dir, f"record_output_{target_version}_run{run_count}")
        if os.path.exists(record_pattern):
            return os.path.basename(record_pattern)
    
    # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå›é€€åˆ°é€šé…ç¬¦åŒ¹é…ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    record_pattern = os.path.join(parent_dir, f"record_output_{target_version}_run*")
    record_folders = glob.glob(record_pattern)
    if record_folders:
        return os.path.basename(record_folders[0])
    return None


def generate_report_name(replay_folder_name):
    """replay_output_* â†’ report_output_*ï¼ˆä»…æ›¿æ¢é¦–ä¸ª 'replay_'ï¼‰"""
    return replay_folder_name.replace("replay_", "report_", 1)


def parse_folder_names(replay_folder_name):
    """
    è§£æ {base_app}, {run_count}, {target_app}
    replay: replay_output_{target_app}_run{run_count}_for_{base_app}
    ä¾‹å¦‚: replay_output_v9_6_0_run1_for_v6_4_1
    - target_app: v9_6_0 (åœ¨replay_output_åé¢ï¼Œ_for_å‰é¢)
    - base_app: v6_4_1 (åœ¨_for_åé¢)
    - run_count: 1 (åœ¨_runåé¢)
    """
    base_app = "unknown"
    target_app = "unknown"
    run_count = "unknown"

    # replay_folder_name: replay_output_v9_6_0_run1_for_v6_4_1
    # æŒ‰ "_for_" åˆ†å‰²
    replay_parts = replay_folder_name.split("_for_")
    if len(replay_parts) == 2:
        # base_app æ˜¯ _for_ åé¢çš„éƒ¨åˆ†
        base_app = replay_parts[1]
        
        # target_app å’Œ run_count åœ¨ _for_ å‰é¢çš„éƒ¨åˆ†
        target_part = replay_parts[0].replace("replay_output_", "")
        # æŒ‰ "_run" åˆ†å‰²
        target_parts = target_part.split("_run")
        if len(target_parts) == 2:
            target_app = target_parts[0]
            run_count = target_parts[1]

    return {
        'base_app': base_app,
        'run_count': run_count,
        'target_app': target_app
    }


def count_replay_events_json(replay_dir):
    """
    ç»Ÿè®¡ replay_dir/events ç›®å½•ä¸‹çš„ .json æ•°é‡ã€‚
    è¿”å› (count, events_dir_exists)
    """
    events_dir = os.path.join(replay_dir, "events")
    if not os.path.isdir(events_dir):
        return 0, False
    json_paths = glob.glob(os.path.join(events_dir, "*.json"))
    return len(json_paths), True


def classify_failure_stage(json_count, events_dir_exists):
    """
    åŸºäº json æ•°é‡åšä¸€ä¸ªç²—ç²’åº¦çš„å¤±è´¥é˜¶æ®µæ¨æ–­ï¼ˆå¯å‘å¼ï¼Œä¾¿äºå¿«é€Ÿå®šä½ï¼‰ï¼š
      - æ—  events ç›®å½•ï¼šno_events_dir
      - 0ï¼šlauncher_failedï¼ˆæœªå¼€å§‹æˆ–ææ—©æœŸå¤±è´¥ï¼‰
      - 1â€“9ï¼švery_early
      - 10â€“49ï¼šearly
      - 50â€“99ï¼šmid
      - >=100ï¼šcompleteï¼ˆè¾¾åˆ°æ—¢å®šæœ€å°é˜ˆå€¼ï¼Œé€šå¸¸è§†ä¸ºè¦†ç›–å……åˆ†ï¼‰
    """
    if not events_dir_exists:
        return "no_events_dir"
    if json_count == 0:
        return "launcher_failed"
    if json_count < 10:
        return "very_early"
    if json_count < 50:
        return "early"
    if json_count < 100:
        return "mid"
    return "complete"


def process_single_replay(args):
    """
    å¤„ç†å•ä¸ªreplayæ–‡ä»¶å¤¹çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†
    """
    (replay_folder, parent_dir, test_mode) = args
    replay_name = os.path.basename(replay_folder)
    
    # æ¨å¯¼ record
    record_name = derive_record_folder(replay_name, parent_dir)
    if not record_name:
        return {
            'base_app': parse_folder_names(replay_name).get('base_app'),
            'run_count': parse_folder_names(replay_name).get('run_count'),
            'target_app': parse_folder_names(replay_name).get('target_app'),
            'replay_dir': replay_name,
            'record_dir': '',
            'report_dir': '',
            'events_json_count': 0,
            'failure_stage': 'unknown',
            'status': 'error',
            'note': 'record_not_found_or_unparsable'
        }
    
    record_path = os.path.join(parent_dir, record_name)
    if not os.path.exists(record_path):
        events_count, events_dir_exists = count_replay_events_json(replay_folder)
        failure_stage = classify_failure_stage(events_count, events_dir_exists)
        return {
            'base_app': parse_folder_names(replay_name).get('base_app'),
            'run_count': parse_folder_names(replay_name).get('run_count'),
            'target_app': parse_folder_names(replay_name).get('target_app'),
            'replay_dir': replay_name,
            'record_dir': record_name,
            'report_dir': '',
            'events_json_count': events_count,
            'failure_stage': failure_stage,
            'status': 'error',
            'note': 'record_missing_on_disk'
        }
    
    # ç”ŸæˆæŠ¥å‘Šç›®å½•å
    report_name = generate_report_name(replay_name)
    report_path = os.path.join(parent_dir, report_name)
    
    # è§£æä¿¡æ¯
    folder_info = parse_folder_names(replay_name)
    
    # ç»Ÿè®¡ events/*.json
    events_count, events_dir_exists = count_replay_events_json(replay_folder)
    failure_stage = classify_failure_stage(events_count, events_dir_exists)
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°100ä¸ªeventsï¼ˆè¡¨ç¤ºæµ‹è¯•å®Œæˆï¼‰â†’ è·³è¿‡ç”Ÿæˆ
    if events_count >= 100:
        return {
            'base_app': folder_info['base_app'],
            'run_count': folder_info['run_count'],
            'target_app': folder_info['target_app'],
            'replay_dir': replay_name,
            'record_dir': record_name,
            'report_dir': report_name,
            'events_json_count': events_count,
            'failure_stage': failure_stage,
            'status': 'skipped',
            'note': 'test_completed_100_events'
        }
    
    # å·²å­˜åœ¨æŠ¥å‘Š â†’ è·³è¿‡ç”Ÿæˆ
    if os.path.exists(report_path):
        return {
            'base_app': folder_info['base_app'],
            'run_count': folder_info['run_count'],
            'target_app': folder_info['target_app'],
            'replay_dir': replay_name,
            'record_dir': record_name,
            'report_dir': report_name,
            'events_json_count': events_count,
            'failure_stage': failure_stage,
            'status': 'skipped',
            'note': 'report_exists'
        }
    
    # ç”ŸæˆæŠ¥å‘Š
    record_path_abs = os.path.abspath(record_path)
    replay_folder_abs = os.path.abspath(replay_folder)
    report_path_abs = os.path.abspath(report_path)
    
    if test_mode:
        # æµ‹è¯•æ¨¡å¼ï¼šä¸å®é™…æ‰§è¡Œ
        return {
            'base_app': folder_info['base_app'],
            'run_count': folder_info['run_count'],
            'target_app': folder_info['target_app'],
            'replay_dir': replay_name,
            'record_dir': record_name,
            'report_dir': report_name,
            'events_json_count': events_count,
            'failure_stage': failure_stage,
            'status': 'test_mode',
            'note': 'test_mode_execution'
        }
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šå®é™…æ‰§è¡Œ
        try:
            result = generate_html_report(record_path_abs, replay_folder_abs, report_path_abs)
            return {
                'base_app': folder_info['base_app'],
                'run_count': folder_info['run_count'],
                'target_app': folder_info['target_app'],
                'replay_dir': replay_name,
                'record_dir': record_name,
                'report_dir': report_name,
                'events_json_count': events_count,
                'failure_stage': failure_stage,
                'status': 'processed',
                'note': ''
            }
        except Exception as e:
            return {
                'base_app': folder_info['base_app'],
                'run_count': folder_info['run_count'],
                'target_app': folder_info['target_app'],
                'replay_dir': replay_name,
                'record_dir': record_name,
                'report_dir': report_name,
                'events_json_count': events_count,
                'failure_stage': failure_stage,
                'status': 'error',
                'note': f'exception: {e}'
            }


def batch_analysis(parent_dir, base_app_filter=None, test_mode=False, parallel=False, max_workers=None):
    """åœ¨ parent_dir ä¸‹æ‰¹é‡åˆ†æï¼ŒæŒ‰ base_app_filterï¼ˆå¯é€‰ï¼‰è¿‡æ»¤"""
    print("Starting batch analysis...")
    print(f"Parent dir: {parent_dir}")
    if base_app_filter:
        print(f"Base app filter: {base_app_filter}")
    if test_mode:
        print("ğŸ§ª TEST MODE: Will show commands instead of executing them")
    if parallel:
        workers = max_workers if max_workers else min(cpu_count(), 4)  # é»˜è®¤æœ€å¤š4ä¸ªè¿›ç¨‹
        print(f"ğŸš€ PARALLEL MODE: Using {workers} workers")

    # æŸ¥æ‰¾ replay
    replay_folders = find_replay_folders(parent_dir, base_app_filter=base_app_filter)

    if not replay_folders:
        print("No replay_output_*_for_* folders found with the given criteria.")
        return

    print(f"Found {len(replay_folders)} replay folders:")
    for folder in replay_folders:
        print(f"  - {os.path.basename(folder)}")

    # å‡†å¤‡å‚æ•°
    process_args = [(replay_folder, parent_dir, test_mode) for replay_folder in replay_folders]

    if parallel and not test_mode:
        # å¹¶è¡Œå¤„ç†
        print(f"\nğŸš€ Processing {len(replay_folders)} folders in parallel...")
        with Pool(processes=workers) as pool:
            analysis_results = pool.map(process_single_replay, process_args)
    else:
        # ä¸²è¡Œå¤„ç†ï¼ˆæµ‹è¯•æ¨¡å¼æˆ–éå¹¶è¡Œæ¨¡å¼ï¼‰
        print(f"\nğŸ”„ Processing {len(replay_folders)} folders sequentially...")
        analysis_results = []
        for i, args in enumerate(process_args, 1):
            replay_folder, parent_dir, test_mode = args
            replay_name = os.path.basename(replay_folder)
            
            if test_mode:
                print(f"[{i}/{len(process_args)}] ğŸ§ª Testing {replay_name}")
            else:
                print(f"[{i}/{len(process_args)}] ğŸ”„ Processing {replay_name}")
            
            result = process_single_replay(args)
            analysis_results.append(result)
            
            # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if test_mode and result['status'] == 'test_mode':
                print(f"   ğŸ§ª TEST MODE - Would execute:")
                print(f"      Python command: python -c \"from droidbot.utils import generate_html_report; generate_html_report('...', '...', '...')\"")
                print(f"      Arguments:")
                print(f"        - replay: {result['replay_dir']}")
                print(f"        - record: {result['record_dir']}")
                print(f"        - events count: {result['events_json_count']}")
                print(f"        - would skip (â‰¥100 events): {'âœ… YES' if result['events_json_count'] >= 100 else 'âŒ NO'}")

    # ç»Ÿè®¡ç»“æœ
    processed_count = sum(1 for r in analysis_results if r['status'] == 'processed')
    skipped_count = sum(1 for r in analysis_results if r['status'] == 'skipped')
    error_count = sum(1 for r in analysis_results if r['status'] == 'error')
    test_mode_count = sum(1 for r in analysis_results if r['status'] == 'test_mode')

    # ç”Ÿæˆ CSVï¼ˆæ”¾åœ¨ parent_dir ä¸‹ï¼‰
    csv_filename = os.path.join(parent_dir, "analysis.csv")
    try:
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'base_app', 'run_count', 'target_app',
                'replay_dir', 'record_dir', 'report_dir',
                'events_json_count', 'failure_stage', 'status', 'note'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for result in analysis_results:
                writer.writerow(result)

        print(f"\nğŸ“Š CSV report generated: {csv_filename}")
        print(f"   - Total records: {len(analysis_results)}")
    except Exception as e:
        print(f"âŒ Error generating CSV: {e}")

    print(f"\nBatch analysis completed:")
    print(f"  - Processed: {processed_count}")
    print(f"  - Skipped: {skipped_count}")
    print(f"  - Errors: {error_count}")
    if test_mode:
        print(f"  - Test mode: {test_mode_count}")
    print(f"  - Total: {len(replay_folders)}")


def main():
    parser = argparse.ArgumentParser(description='Generate HTML report for DroidBot test results')
    parser.add_argument('--batch', action='store_true', help='Run batch analysis on replay folders')
    parser.add_argument('--test-mode', action='store_true', help='Test mode: show commands instead of executing them (only works with --batch)')
    parser.add_argument('--parallel', action='store_true', help='Enable parallel processing for batch mode (faster but uses more CPU)')
    parser.add_argument('--max-workers', type=int, default=None,
                        help='Maximum number of parallel workers (default: min(CPU cores, 4))')
    parser.add_argument('--deduplicate', action='store_true', help='Enable deduplication mode to find unique test cases')
    parser.add_argument('--run-count', default=None, help='Filter by specific run count for deduplication (e.g., "1")')
    parser.add_argument('--parent-dir', default=os.getcwd(),
                        help='Root directory to search for record/replay/report folders (default: CWD)')
    parser.add_argument('--base-app', default=None,
                        help='Filter replay folders by base app version (e.g., "3_8_0" to match *_for_3_8_0)')
    parser.add_argument('output_dir', nargs='?', help='Directory containing record data (original test output)')
    parser.add_argument('replay_output_dir', nargs='?', help='Directory containing replay data (optional)')
    parser.add_argument('out_dir', nargs='?', help='Output directory for the complete report (HTML + images)')

    args = parser.parse_args()

    parent_dir = os.path.abspath(args.parent_dir)

    if args.batch:
        # éªŒè¯test-modeåªèƒ½ä¸batchä¸€èµ·ä½¿ç”¨
        if args.test_mode and not args.batch:
            parser.error("--test-mode can only be used with --batch")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å»é‡æ¨¡å¼
        if args.deduplicate:
            print("ğŸ” Running in deduplication mode...")
            deduplicated_results = de_duplicate_case(
                parent_dir=parent_dir, 
                base_app_filter=args.base_app, 
                run_count_filter=args.run_count
            )
            
            # ç”Ÿæˆå»é‡åçš„CSVæŠ¥å‘Š
            csv_filename = os.path.join(parent_dir, "deduplicated_analysis.csv")
            try:
                with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = [
                        'base_app', 'run_count', 'target_app',
                        'replay_dir', 'events_json_count', 'failure_stage', 
                        'group_key', 'is_unique'
                    ]
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    for result in deduplicated_results:
                        # è§£æbase_appå’Œrun_count
                        folder_info = parse_folder_names(result['replay_name'])
                        writer.writerow({
                            'base_app': folder_info['base_app'],
                            'run_count': folder_info['run_count'],
                            'target_app': result['target_app'],
                            'replay_dir': result['replay_name'],
                            'events_json_count': result['events_count'],
                            'failure_stage': result['failure_stage'],
                            'group_key': result['group_key'],
                            'is_unique': result['is_unique']
                        })
                
                print(f"\nğŸ“Š Deduplicated CSV report generated: {csv_filename}")
                print(f"   - Unique cases: {len(deduplicated_results)}")
            except Exception as e:
                print(f"âŒ Error generating deduplicated CSV: {e}")
            
            # ä¸ºå»é‡åçš„ç»“æœç”ŸæˆHTMLæŠ¥å‘Š
            if deduplicated_results:
                print(f"\nğŸ”„ Generating HTML reports for {len(deduplicated_results)} unique cases...")
                generate_deduplicated_reports(deduplicated_results, parent_dir, args.parallel, args.max_workers)
        else:
            # æ­£å¸¸batchæ¨¡å¼
            batch_analysis(parent_dir=parent_dir, base_app_filter=args.base_app, test_mode=args.test_mode, 
                          parallel=args.parallel, max_workers=args.max_workers)
    else:
        # å•ä¸ªæŠ¥å‘Šç”Ÿæˆï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        if not args.output_dir:
            parser.error("output_dir is required when not using --batch mode")

        output_dir = os.path.abspath(args.output_dir)
        replay_output_dir = os.path.abspath(args.replay_output_dir) if args.replay_output_dir else None
        out_dir = os.path.abspath(args.out_dir) if args.out_dir else None

        # å•ä¸ªä¹Ÿç»Ÿè®¡ä¸€æ¬¡ events_jsonï¼ˆå¦‚æœç”¨æˆ·ç»™äº† replay_output_dirï¼‰
        events_count, events_dir_exists = (0, False)
        if replay_output_dir:
            events_count, events_dir_exists = count_replay_events_json(replay_output_dir)
            stage = classify_failure_stage(events_count, events_dir_exists)
            print(f"[Single] events_json_count={events_count}, failure_stage={stage}")

        result = generate_html_report(output_dir, replay_output_dir, out_dir)
        print(f"Report generated successfully: {result}")


def read_version_order_from_csv(parent_dir):
    """
    ä»select_apksç›®å½•è¯»å–CSVæ–‡ä»¶ï¼Œè·å–ç‰ˆæœ¬çš„æ—¶é—´é¡ºåº
    ä½¿ç”¨ä¸start_bash.pyç›¸åŒçš„æ–¹æ³•ï¼šç›´æ¥è¯»å–ç¬¬7åˆ—ï¼ˆç´¢å¼•6ï¼‰ï¼Œè·³è¿‡å‰ä¸¤è¡Œ
    ä¾‹å¦‚ï¼šparent_diræ˜¯com.byagowi.persiancalendarï¼Œåˆ™è¯»å–select_apks/com.byagowi.persiancalendar.csv
    """
    # æ„å»ºCSVæ–‡ä»¶è·¯å¾„ 
    app_name = parent_dir.split("/")[-1]
    csv_path = os.path.join("droidbot/select_apks/", f"{app_name}.csv")
    
    if not os.path.exists(csv_path):
        print(f"âš ï¸  CSV file not found: {csv_path}")
        return None

    
    try:
        with open(csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            rows = list(reader)
        
        versions = []
        for row in rows[1:]:
            if len(row) > 6 and row[6].strip():
                version = row[6].strip()
                # å»æ‰.apkåç¼€ï¼Œåªä¿ç•™ç‰ˆæœ¬å·
                if version.endswith('.apk'):
                    version = version[:-4]
                versions.append(version)
        
        versions.reverse()  # from old to new
        return versions
            
    except Exception as e:
        print(f"âŒ Error reading CSV: {e}")
        return None


def de_duplicate_case(parent_dir, base_app_filter=None, run_count_filter=None):
    """
    å»é‡æŠ¥å‘Š - æ ¹æ®events_json_countçš„å˜åŒ–è¯†åˆ«æœ‰æ„ä¹‰çš„æµ‹è¯•æ¡ˆä¾‹
    
    1. ä»select_apksç›®å½•è¯»å–CSVæ–‡ä»¶ï¼Œè·å–ç‰ˆæœ¬çš„æ—¶é—´é¡ºåº
    2. å›ºå®šä¸€ä¸ªrun countï¼Œæ‰¾åˆ°è¿™äº›ç‰ˆæœ¬çš„replayä¸­çš„events_json_count
    3. å–ç¬¬ä¸€ä¸ªä¸ç›¸åŒçš„events_json_countï¼Œç”Ÿæˆå¯¹åº”çš„æŠ¥å‘Š
    
    ä¾‹å¦‚ï¼šbase appæ˜¯v6.4.1ï¼Œtarget appæŒ‰æ—¶é—´é¡ºåºæ˜¯v6.4.2, v6.4.3, v6.4.4, v6.4.5, v6.4.6, v6.4.7
    events_json_count: 51, 49, 49, 32, 7, 7
    ç»“æœï¼šéœ€è¦ç”ŸæˆæŠ¥å‘Šçš„target appåºå·æ˜¯1,2,4,5 (ç¬¬ä¸€ä¸ªä¸ç›¸åŒçš„events_json_count)
    """
    print("ğŸ” Starting deduplication analysis...")
    print(f"Parent dir: {parent_dir}")
    if base_app_filter:
        print(f"Base app filter: {base_app_filter}")
    if run_count_filter:
        print(f"Run count filter: {run_count_filter}")
    else:
        print("Run count filter: None (processing all run counts)")
    
    # ç¬¬ä¸€æ­¥ï¼šä»CSVè¯»å–ç‰ˆæœ¬é¡ºåº
    csv_versions = read_version_order_from_csv(parent_dir)
    if not csv_versions:
        print("âŒ Cannot proceed without version order from CSV")
        return []
    
    # ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾æ‰€æœ‰replayæ–‡ä»¶å¤¹
    replay_folders = find_replay_folders(parent_dir, base_app_filter=base_app_filter)
    print(f"Found {len(replay_folders)} replay folders:")
    print(f"{replay_folders[0]}")
    
    if not replay_folders:
        print("No replay_output_*_for_* folders found.")
        return []
    
    # ç¬¬ä¸‰æ­¥ï¼šæŒ‰base_appå’Œrun_countåˆ†ç»„ï¼Œå¹¶æŒ‰CSVç‰ˆæœ¬é¡ºåºæ’åº
    groups = {}
    for replay_folder in replay_folders:
        replay_name = os.path.basename(replay_folder)
        folder_info = parse_folder_names(replay_name)
        
        base_app = folder_info['base_app']
        run_count = folder_info['run_count']
        target_app = folder_info['target_app']

        # å¦‚æœæŒ‡å®šäº†run_count_filterï¼Œåªå¤„ç†åŒ¹é…çš„ï¼ˆé»˜è®¤å¤„ç†æ‰€æœ‰run countï¼‰
        if run_count_filter and str(run_count) != str(run_count_filter):
            continue
            
        # å¦‚æœæŒ‡å®šäº†base_app_filterï¼Œåªå¤„ç†åŒ¹é…çš„
        if base_app_filter and base_app != base_app_filter:
            continue
        
        key = f"{base_app}_run{run_count}"
        if key not in groups:
            groups[key] = []
        
        # ç»Ÿè®¡eventsæ•°é‡
        events_count, events_dir_exists = count_replay_events_json(replay_folder)
        
        groups[key].append({
            'replay_folder': replay_folder,
            'replay_name': replay_name,
            'target_app': target_app,
            'events_count': events_count,
            'events_dir_exists': events_dir_exists,
            'failure_stage': classify_failure_stage(events_count, events_dir_exists)
        })
    
    print(f"Found {len(groups)} groups to analyze:")
    for key, items in groups.items():
        print(f"  - {key}: {len(items)} target apps")
    
    # ç¬¬å››æ­¥ï¼šå¯¹æ¯ä¸ªç»„è¿›è¡Œå»é‡åˆ†æ
    deduplicated_results = []
    
    for group_key, items in groups.items():
        print(f"\nğŸ“Š Analyzing group: {group_key}")

        # print(f"CSV versions: {csv_versions}")
        
        # æŒ‰CSVä¸­çš„ç‰ˆæœ¬é¡ºåºæ’åº
        def get_version_order(item):
            target_app = item['target_app']
            # é€†å‘sanitize_suffixï¼šå°†ä¸‹åˆ’çº¿ç‰ˆæœ¬è½¬æ¢ä¸ºç‚¹å·ç‰ˆæœ¬è¿›è¡ŒåŒ¹é…
            # ä¾‹å¦‚ï¼šv9_9_1 â†’ v9.9.1
            target_app_dots = target_app.replace('_', '.')
            try:
                return csv_versions.index(target_app_dots)
            except ValueError:
                # å¦‚æœç‰ˆæœ¬ä¸åœ¨CSVä¸­ï¼Œæ”¾åˆ°æœ€å
                return len(csv_versions)
        
        items.sort(key=get_version_order)
        
        print(f"Target apps in CSV order: {[item['target_app'] for item in items]}")
        print(f"Events counts: {[item['events_count'] for item in items]}")
        
        # å»é‡é€»è¾‘ï¼šå–ç¬¬ä¸€ä¸ªä¸ç›¸åŒçš„events_json_count
        unique_indices = []
        last_events_count = None
        
        for i, item in enumerate(items):
            current_events_count = item['events_count']
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªï¼Œæˆ–è€…events_countä¸ä¸Šä¸€ä¸ªä¸åŒï¼Œåˆ™ä¿ç•™
            if last_events_count is None or current_events_count != last_events_count:
                unique_indices.append(i)
                last_events_count = current_events_count
                print(f"  âœ… Keep {item['target_app']} (events: {current_events_count})")
            else:
                print(f"  â­ï¸  Skip {item['target_app']} (events: {current_events_count}, same as previous)")
        
        # å°†å»é‡åçš„ç»“æœæ·»åŠ åˆ°æœ€ç»ˆåˆ—è¡¨
        for idx in unique_indices:
            item = items[idx]
            deduplicated_results.append({
                'replay_folder': item['replay_folder'],
                'replay_name': item['replay_name'],
                'target_app': item['target_app'],
                'events_count': item['events_count'],
                'events_dir_exists': item['events_dir_exists'],
                'failure_stage': item['failure_stage'],
                'group_key': group_key,
                'is_unique': True
            })
    
    print(f"\nğŸ¯ Deduplication completed:")
    print(f"  - Original total: {sum(len(items) for items in groups.values())}")
    print(f"  - Unique cases: {len(deduplicated_results)}")
    print(f"  - Reduction: {sum(len(items) for items in groups.values()) - len(deduplicated_results)} cases removed")
    
    return deduplicated_results


def generate_deduplicated_reports(deduplicated_results, parent_dir, parallel=False, max_workers=None):
    """
    ä¸ºå»é‡åçš„ç»“æœç”ŸæˆHTMLæŠ¥å‘Š
    """
    print(f"ğŸ”„ Generating HTML reports for {len(deduplicated_results)} unique cases...")
    
    if parallel:
        workers = max_workers if max_workers else min(cpu_count(), 4)
        print(f"ğŸš€ PARALLEL MODE: Using {workers} workers")
    
    # å‡†å¤‡å‚æ•°
    process_args = []
    for result in deduplicated_results:
        replay_folder = result['replay_folder']
        replay_name = result['replay_name']
        
        # æ¨å¯¼recordæ–‡ä»¶å¤¹
        record_name = derive_record_folder(replay_name, parent_dir)
        if not record_name:
            print(f"âš ï¸  Could not derive record folder for {replay_name}")
            continue
            
        record_path = os.path.join(parent_dir, record_name)
        if not os.path.exists(record_path):
            print(f"âš ï¸  Record folder not found: {record_name}")
            continue
        
        # ç”ŸæˆæŠ¥å‘Šç›®å½•å
        report_name = generate_report_name(replay_name)
        report_path = os.path.join(parent_dir, report_name)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æŠ¥å‘Š
        if os.path.exists(report_path):
            print(f"â­ï¸  Skipping {replay_name} - report already exists: {report_name}")
            continue
        
        process_args.append((replay_folder, record_path, report_path))
    
    if not process_args:
        print("No reports to generate.")
        return
    
    print(f"Found {len(process_args)} reports to generate")
    
    if parallel:
        # å¹¶è¡Œå¤„ç†
        print(f"ğŸš€ Processing {len(process_args)} reports in parallel...")
        with Pool(processes=workers) as pool:
            results = pool.map(generate_single_report, process_args)
    else:
        # ä¸²è¡Œå¤„ç†
        print(f"ğŸ”„ Processing {len(process_args)} reports sequentially...")
        results = []
        for i, args in enumerate(process_args, 1):
            print(f"[{i}/{len(process_args)}] ğŸ”„ Processing {os.path.basename(args[0])}")
            result = generate_single_report(args)
            results.append(result)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r['status'] == 'success')
    error_count = sum(1 for r in results if r['status'] == 'error')
    
    print(f"\nHTML report generation completed:")
    print(f"  - Success: {success_count}")
    print(f"  - Errors: {error_count}")
    print(f"  - Total: {len(process_args)}")


def generate_single_report(args):
    """
    ç”Ÿæˆå•ä¸ªHTMLæŠ¥å‘Šçš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†
    """
    replay_folder, record_path, report_path = args
    
    try:
        result = generate_html_report(record_path, replay_folder, report_path)
        return {
            'status': 'success',
            'replay_folder': os.path.basename(replay_folder),
            'report_path': os.path.basename(report_path),
            'result': result
        }
    except Exception as e:
        return {
            'status': 'error',
            'replay_folder': os.path.basename(replay_folder),
            'report_path': os.path.basename(report_path),
            'error': str(e)
        }


if __name__ == "__main__":
    main()


# python check_result.py --batch --deduplicate --base-app v6_4_1 --parent-dir com.byagowi.persiancalendar/ --parallel --max-workers 10